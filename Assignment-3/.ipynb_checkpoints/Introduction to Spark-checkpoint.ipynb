{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook shows introduces the basic concepts of RDDs and operations on them visually, by showing the contents of the RDDs as a table.\n",
    "\n",
    "**Note: If you are looking at this in GitHub, you may not be able to see the HTML tables. Make sure to use the nbviewer link: http://nbviewer.jupyter.org/github/umddb/cmsc424-fall2016/tree/master/**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Apache Spark is a relatively new cluster computing framework, developed originally at UC Berkeley. It significantly generalizes the 2-stage Map-Reduce paradigm (originally proposed by Google and popularized by open-source Hadoop system); Spark is instead based on the abstraction of **resilient distributed datasets (RDDs)**. An RDD is basically a distributed collection of items, that can be created in a variety of ways. Spark provides a set of operations to transform one or more RDDs into an output RDD, and analysis tasks are written as chains of these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display RDD\n",
    "The following helper functions displays the current contents of an RDD (partition-by-partition). This is best used for small RDDs with manageable number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayRDD:\n",
    "        def __init__(self, rdd):\n",
    "                self.rdd = rdd\n",
    "\n",
    "        def _repr_html_(self):                                  \n",
    "                x = self.rdd.mapPartitionsWithIndex(lambda i, x: [(i, [y for y in x])])\n",
    "                l = x.collect()\n",
    "                s = \"<table><tr>{}</tr><tr><td>\".format(\"\".join([\"<th>Partition {}\".format(str(j)) for (j, r) in l]))\n",
    "                s += '</td><td valign=\"bottom\" halignt=\"left\">'.join([\"<ul><li>{}</ul>\".format(\"<li>\".join([str(rr) for rr in r])) for (j, r) in l])\n",
    "                s += \"</td></table>\"\n",
    "                return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Basics 1\n",
    "Lets start with some basic operations using a small RDD to visualize what's going on. We will create a RDD of Strings, using the `states.txt` file which contains a list of the state names.\n",
    "\n",
    "The notebook has already initialized a SparkContext, and we can refer to it as `sc`.\n",
    "\n",
    "We will use `sc.textFile` to create this RDD. This operations reads the file and treats every line as a separate object. We will use DisplayRDD() to visualize it. The second argument of `sc.textFile` is the number of partitions. We will set this as 10 to get started. If we don't do that, Spark will only create a single partition given the file is pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5<th>Partition 6<th>Partition 7<th>Partition 8<th>Partition 9</tr><tr><td><ul><li>Alabama<li>Hawaii<li>Massachusetts<li>New Mexico<li>South Dakota</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Alaska<li>Idaho<li>Michigan<li>New York<li>Tennessee<li>Arizona</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Illinois<li>Minnesota<li>North Carolina<li>Texas</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Arkansas<li>Indiana<li>Mississippi<li>North Dakota<li>Utah</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>California<li>Iowa<li>Missouri<li>Ohio<li>Vermont<li>Colorado</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Kansas<li>Montana<li>Oklahoma<li>Virginia<li>Connecticut<li>Kentucky</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Nebraska<li>Oregon<li>Washington<li>Delaware<li>Louisiana</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Nevada<li>Pennsylvania<li>West Virginia<li>Florida</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Maine<li>New Hampshire<li>Rhode Island<li>Wisconsin<li>Georgia</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Maryland<li>New Jersey<li>South Carolina<li>Wyoming</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffffa6ad7790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_rdd = sc.textFile('states.txt', 10)\n",
    "DisplayRDD(states_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the contents of each partition as a list -- so the first Partition has 5 elements in it ('Alabama', ...). We can `repartition` the RDD to get a fewer partitions so it will be easier to see. \n",
    "\n",
    "Note: There is some randomness in this process, so the result may vary if you repeat the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Kansas<li>Montana<li>Oklahoma<li>Virginia<li>Connecticut<li>Kentucky<li>Nebraska<li>Oregon<li>Washington<li>Delaware<li>Louisiana<li>Nevada<li>Pennsylvania<li>West Virginia<li>Florida<li>Maine<li>New Hampshire<li>Rhode Island<li>Wisconsin<li>Georgia</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Arkansas<li>Indiana<li>Mississippi<li>North Dakota<li>Utah</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Alabama<li>Hawaii<li>Massachusetts<li>New Mexico<li>South Dakota<li>Alaska<li>Idaho<li>Michigan<li>New York<li>Tennessee<li>Arizona<li>Illinois<li>Minnesota<li>North Carolina<li>Texas<li>California<li>Iowa<li>Missouri<li>Ohio<li>Vermont<li>Colorado<li>Maryland<li>New Jersey<li>South Carolina<li>Wyoming</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bcbbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_rdd = states_rdd.repartition(5)\n",
    "DisplayRDD(states_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a transformation where we convert a string to a 2-tuple, where the second value is the length of the string. We will just use a `map` for this -- we have to provide a function as the input that transforms each element of the RDD. In this case, we are using the `lambda` keyword to define a function inline. See here: https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/ for a tutorial on lambda functions.\n",
    "\n",
    "The below lambda function is simply taking in a string: s, and returning a 2-tuple: (s, len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('Kansas', 6)<li>('Montana', 7)<li>('Oklahoma', 8)<li>('Virginia', 8)<li>('Connecticut', 11)<li>('Kentucky', 8)<li>('Nebraska', 8)<li>('Oregon', 6)<li>('Washington', 10)<li>('Delaware', 8)<li>('Louisiana', 9)<li>('Nevada', 6)<li>('Pennsylvania', 12)<li>('West Virginia', 13)<li>('Florida', 7)<li>('Maine', 5)<li>('New Hampshire', 13)<li>('Rhode Island', 12)<li>('Wisconsin', 9)<li>('Georgia', 7)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('Arkansas', 8)<li>('Indiana', 7)<li>('Mississippi', 11)<li>('North Dakota', 12)<li>('Utah', 4)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('Alabama', 7)<li>('Hawaii', 6)<li>('Massachusetts', 13)<li>('New Mexico', 10)<li>('South Dakota', 12)<li>('Alaska', 6)<li>('Idaho', 5)<li>('Michigan', 8)<li>('New York', 8)<li>('Tennessee', 9)<li>('Arizona', 7)<li>('Illinois', 8)<li>('Minnesota', 9)<li>('North Carolina', 14)<li>('Texas', 5)<li>('California', 10)<li>('Iowa', 4)<li>('Missouri', 8)<li>('Ohio', 4)<li>('Vermont', 7)<li>('Colorado', 8)<li>('Maryland', 8)<li>('New Jersey', 10)<li>('South Carolina', 14)<li>('Wyoming', 7)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffffa6ad7940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states1 = states_rdd.map(lambda s: (s, len(s)))\n",
    "DisplayRDD(states1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets collect all the names with the same length together using a group by operation. \n",
    "```\n",
    "groupByKey([numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. \n",
    "```\n",
    "This wouldn't work as is, because `states1` is using the name as the key. Let's change that around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, 'Kansas')<li>(7, 'Montana')<li>(8, 'Oklahoma')<li>(8, 'Virginia')<li>(11, 'Connecticut')<li>(8, 'Kentucky')<li>(8, 'Nebraska')<li>(6, 'Oregon')<li>(10, 'Washington')<li>(8, 'Delaware')<li>(9, 'Louisiana')<li>(6, 'Nevada')<li>(12, 'Pennsylvania')<li>(13, 'West Virginia')<li>(7, 'Florida')<li>(5, 'Maine')<li>(13, 'New Hampshire')<li>(12, 'Rhode Island')<li>(9, 'Wisconsin')<li>(7, 'Georgia')</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, 'Arkansas')<li>(7, 'Indiana')<li>(11, 'Mississippi')<li>(12, 'North Dakota')<li>(4, 'Utah')</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, 'Alabama')<li>(6, 'Hawaii')<li>(13, 'Massachusetts')<li>(10, 'New Mexico')<li>(12, 'South Dakota')<li>(6, 'Alaska')<li>(5, 'Idaho')<li>(8, 'Michigan')<li>(8, 'New York')<li>(9, 'Tennessee')<li>(7, 'Arizona')<li>(8, 'Illinois')<li>(9, 'Minnesota')<li>(14, 'North Carolina')<li>(5, 'Texas')<li>(10, 'California')<li>(4, 'Iowa')<li>(8, 'Missouri')<li>(4, 'Ohio')<li>(7, 'Vermont')<li>(8, 'Colorado')<li>(8, 'Maryland')<li>(10, 'New Jersey')<li>(14, 'South Carolina')<li>(7, 'Wyoming')</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bca530>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states2 = states1.map(lambda t: (t[1], t[0]))\n",
    "DisplayRDD(states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that Spark did not do a shuffle to ensure that the same `keys` end up on the same partition. In fact, the `map` operation does not do a shuffle. \n",
    "\n",
    "Now we can do a groupByKey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li>(10, <pyspark.resultiterable.ResultIterable object at 0xffff82bc96f0>)<li>(5, <pyspark.resultiterable.ResultIterable object at 0xffff82bc97b0>)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, <pyspark.resultiterable.ResultIterable object at 0xffff82bc9810>)<li>(11, <pyspark.resultiterable.ResultIterable object at 0xffff82bc9870>)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, <pyspark.resultiterable.ResultIterable object at 0xffff82bc98d0>)<li>(12, <pyspark.resultiterable.ResultIterable object at 0xffff82bc9930>)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, <pyspark.resultiterable.ResultIterable object at 0xffff82bc9990>)<li>(13, <pyspark.resultiterable.ResultIterable object at 0xffff82bcab30>)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(9, <pyspark.resultiterable.ResultIterable object at 0xffff82bcbd60>)<li>(4, <pyspark.resultiterable.ResultIterable object at 0xffff82bca260>)<li>(14, <pyspark.resultiterable.ResultIterable object at 0xffff82bca200>)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bcb8b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states3 = states2.groupByKey()\n",
    "DisplayRDD(states3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks weird... it seems to have done a group by, but we are missing the groups themselves. This is because the type of the value is a `pyspark.resultiterable.ResultIterable` which our DisplayRDD code does not translate into strings. We can fix that by converting the `values` to lists, and then doing DisplayRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li>(10, ['Washington', 'New Mexico', 'California', 'New Jersey'])<li>(5, ['Maine', 'Idaho', 'Texas'])</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, ['Kansas', 'Oregon', 'Nevada', 'Hawaii', 'Alaska'])<li>(11, ['Connecticut', 'Mississippi'])</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, ['Montana', 'Florida', 'Georgia', 'Indiana', 'Alabama', 'Arizona', 'Vermont', 'Wyoming'])<li>(12, ['Pennsylvania', 'Rhode Island', 'North Dakota', 'South Dakota'])</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, ['Oklahoma', 'Virginia', 'Kentucky', 'Nebraska', 'Delaware', 'Arkansas', 'Michigan', 'New York', 'Illinois', 'Missouri', 'Colorado', 'Maryland'])<li>(13, ['West Virginia', 'New Hampshire', 'Massachusetts'])</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(9, ['Louisiana', 'Wisconsin', 'Tennessee', 'Minnesota'])<li>(4, ['Utah', 'Iowa', 'Ohio'])<li>(14, ['North Carolina', 'South Carolina'])</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bca500>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(states3.mapValues(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it goes. Now we can see that the operation properly grouped together the state names by their lengths. This operation required a `shuffle` since originally all names with length, say 10, were all over the place.\n",
    "\n",
    "`groupByKey` does not reduce the size of the RDD. If we were interested in `counting` the number of states with a given length (i.e., a `group by count` query), we can use `reduceByKey` instead. However that requires us to do a map first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, 1)<li>(7, 1)<li>(8, 1)<li>(8, 1)<li>(11, 1)<li>(8, 1)<li>(8, 1)<li>(6, 1)<li>(10, 1)<li>(8, 1)<li>(9, 1)<li>(6, 1)<li>(12, 1)<li>(13, 1)<li>(7, 1)<li>(5, 1)<li>(13, 1)<li>(12, 1)<li>(9, 1)<li>(7, 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, 1)<li>(7, 1)<li>(11, 1)<li>(12, 1)<li>(4, 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, 1)<li>(6, 1)<li>(13, 1)<li>(10, 1)<li>(12, 1)<li>(6, 1)<li>(5, 1)<li>(8, 1)<li>(8, 1)<li>(9, 1)<li>(7, 1)<li>(8, 1)<li>(9, 1)<li>(14, 1)<li>(5, 1)<li>(10, 1)<li>(4, 1)<li>(8, 1)<li>(4, 1)<li>(7, 1)<li>(8, 1)<li>(8, 1)<li>(10, 1)<li>(14, 1)<li>(7, 1)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82940580>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states4 = states2.mapValues(lambda x: 1)\n",
    "DisplayRDD(states4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey` takes in a single reduce function as the input which tells us what to do with any two values. In this case, we are simply going to use sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li>(10, 4)<li>(5, 3)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, 5)<li>(11, 2)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, 8)<li>(12, 4)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, 12)<li>(13, 3)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(9, 4)<li>(4, 3)<li>(14, 2)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bca080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(states4.reduceByKey(lambda v1, v2: v1 + v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations could be done faster through using `aggregateByKey`, but the syntax takes some getting used to. `aggregateByKey` takes a `start` value, a function that tells it what to do for a given element in the RDD, and another reduce function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li>(10, 4)<li>(5, 3)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(6, 5)<li>(11, 2)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(7, 8)<li>(12, 4)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(8, 12)<li>(13, 3)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>(9, 4)<li>(4, 3)<li>(14, 2)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bc9840>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(states2.aggregateByKey(0, lambda k, v: k+1, lambda v1, v2: v1+v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics 2: FlatMap\n",
    "\n",
    "Unlike a `map`, the function used for `flatMap` returns a list -- this is used to allow for the possibility that we will generate different numbers of outputs for different elements. Here is an example where we split each string in `states_rdd` into multiple substrings.\n",
    "\n",
    "The lambda function below splits a string into chunks of size 5: so 'South Dakota' gets split into 'South', ' Dako', 'ta', and so on. The lambda function itself returns a list. If you try this with 'map' the result would not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Kansa<li>s<li>Monta<li>na<li>Oklah<li>oma<li>Virgi<li>nia<li>Conne<li>cticu<li>t<li>Kentu<li>cky<li>Nebra<li>ska<li>Orego<li>n<li>Washi<li>ngton<li>Delaw<li>are<li>Louis<li>iana<li>Nevad<li>a<li>Penns<li>ylvan<li>ia<li>West <li>Virgi<li>nia<li>Flori<li>da<li>Maine<li>New H<li>ampsh<li>ire<li>Rhode<li> Isla<li>nd<li>Wisco<li>nsin<li>Georg<li>ia</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Arkan<li>sas<li>India<li>na<li>Missi<li>ssipp<li>i<li>North<li> Dako<li>ta<li>Utah</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Alaba<li>ma<li>Hawai<li>i<li>Massa<li>chuse<li>tts<li>New M<li>exico<li>South<li> Dako<li>ta<li>Alask<li>a<li>Idaho<li>Michi<li>gan<li>New Y<li>ork<li>Tenne<li>ssee<li>Arizo<li>na<li>Illin<li>ois<li>Minne<li>sota<li>North<li> Caro<li>lina<li>Texas<li>Calif<li>ornia<li>Iowa<li>Misso<li>uri<li>Ohio<li>Vermo<li>nt<li>Color<li>ado<li>Maryl<li>and<li>New J<li>ersey<li>South<li> Caro<li>lina<li>Wyomi<li>ng</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bc96c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(states_rdd.flatMap(lambda x: [str(x[i:i+5]) for i in range(0, len(x), 5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics 3: Joins\n",
    "\n",
    "Finally, lets look at an example of joins. We will still use small RDDs, but we now need two of them. We will just use `sc.parallelize` to create those RDDs. That functions takes in a list and creates an RDD of that by creating partitions and splitting them across machines. It takes the number of partitions as the second argument (optional).\n",
    "\n",
    "Note again that Spark made no attempt to co-locate the objects (i.e., the tuples) with the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2</tr><tr><td><ul><li>('alpha', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('beta', 2)<li>('gamma', 3)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('alpha', 5)<li>('beta', 6)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bc9180>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('alpha', 1), ('beta', 2), ('gamma', 3), ('alpha', 5), ('beta', 6)], 3)\n",
    "DisplayRDD(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2</tr><tr><td><ul><li>('alpha', 'South Dakota')</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('beta', 'North Dakota')</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('zeta', 'Maryland')<li>('beta', 'Washington')</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bc9510>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([('alpha', 'South Dakota'), ('beta', 'North Dakota'), ('zeta', 'Maryland'), ('beta', 'Washington')], 3)\n",
    "DisplayRDD(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the definition of join from the programming guide.\n",
    "```\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin. \n",
    "```\n",
    "We want to join on the first attributes, so we can just call join directly, otherwise a map may have been required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('alpha', (1, 'South Dakota'))<li>('alpha', (5, 'South Dakota'))<li>('beta', (2, 'North Dakota'))<li>('beta', (2, 'Washington'))<li>('beta', (6, 'North Dakota'))<li>('beta', (6, 'Washington'))</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bca050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd1.join(rdd2)\n",
    "DisplayRDD(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bunch of empty partitions. We could have controlled the number of partitions with an optional argument to join. But in any case, the output looks like what we were trying to do. Using `outerjoins` behaves as you would expect, with two extra tuples for fullOuterJoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('zeta', (None, 'Maryland'))</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('gamma', (3, None))</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('alpha', (1, 'South Dakota'))<li>('alpha', (5, 'South Dakota'))<li>('beta', (2, 'North Dakota'))<li>('beta', (2, 'Washington'))<li>('beta', (6, 'North Dakota'))<li>('beta', (6, 'Washington'))</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82bcb3a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(rdd1.fullOuterJoin(rdd2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cogroup` is a related function, but basically creates two lists with each key. The `value` in that case is more complex, and our code above can't handle it. As we can see, there is a single object corresponding to each key, and the values are basically a pair of `iterables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5</tr><tr><td><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('zeta', ([], ['Maryland']))</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('gamma', ([3], []))</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li></ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('alpha', ([1, 5], ['South Dakota']))<li>('beta', ([2, 6], ['North Dakota', 'Washington']))</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82942d40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayRDD(rdd1.cogroup(rdd2).mapValues(lambda x: (list(x[0]), list(x[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics 4\n",
    "\n",
    "Here we will run some of the commands from the README file. This uses an RDD created from the lines of README.md file. You can use the DisplayRDD function here, but the output is rather large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"README.md\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Assignment 4: Apache Spark',\n",
       " '',\n",
       " \"The goal of this assignment is to learn how to do large-scale data analysis tasks using Apache Spark: for this assignment, we will use relatively small datasets and  we won't run anything in distributed mode; however Spark can be easily used to run the same programs on much larger datasets.\",\n",
       " '',\n",
       " '### Getting Started with Spark']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the README file, the following command does a word count, by first separating out the words using a `flatMap`, and then using a `reduceByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5<th>Partition 6<th>Partition 7<th>Partition 8<th>Partition 9</tr><tr><td><ul><li>('Assignment', 2)<li>('', 91)<li>('The', 20)<li>('of', 55)<li>('is', 36)<li>('run', 4)<li>('anything', 1)<li>('basically', 2)<li>('at', 6)<li>('[Apache', 1)<li>('(originally', 1)<li>('instead', 1)<li>('an', 17)<li>('output', 7)<li>('HDFS', 1)<li>('1.', 2)<li>('**Version', 1)<li>('have', 7)<li>('there,', 1)<li>('ready', 1)<li>('languages:', 1)<li>('quick', 2)<li>('PYSPARK_DRIVER_PYTHON_OPTS=\"notebook', 1)<li>('--no-browser', 1)<li>('Shell', 2)<li>('relevant', 1)<li>('python', 3)<li>('sc.textFile(\"README.md\")`:', 1)<li>('commands', 3)<li>('line', 2)<li>('RDD).', 1)<li>('(http://spark.apache.org/docs/latest/quick-start.html).', 1)<li>('following', 4)<li>('appears', 1)<li>('Use', 6)<li>('class,', 1)<li>('preferable,', 1)<li>('especially', 1)<li>('mode.', 1)<li>('look', 2)<li>('Programming', 1)<li>('Details', 1)<li>('initializes', 1)<li>('*', 6)<li>('consisting', 6)<li>('tuple', 2)<li>('documents', 1)<li>('inspect', 1)<li>('defined', 1)<li>('would', 8)<li>('(0.25)**:', 16)<li>('no', 2)<li>('year', 3)<li>('`moviesRDD`.', 1)<li>('(genre,', 1)<li>('two', 6)<li>('tag', 1)<li>('before', 3)<li>('applying', 1)<li>('((7,', 3)<li>('input', 5)<li>('was', 1)<li>('Easiest', 1)<li>('[(0,', 1)<li>('10', 1)<li>('everything', 1)<li>('not\",', 1)<li>('characters', 1)<li>('-->', 3)<li>(\"'before',\", 1)<li>('50', 1)<li>('score', 1)<li>('`postsRDD`', 1)<li>('purpose,', 1)<li>('Output', 1)<li>('count', 1)<li>('score.', 1)<li>('answers:', 1)<li>('197,', 1)<li>('starting', 1)<li>('category', 1)<li>('(just', 1)<li>('format', 2)<li>('Try', 1)<li>('than', 1)<li>('analogous', 1)<li>('host,', 1)<li>('host', 2)<li>('something', 1)<li>('(with', 2)<li>('initial', 1)<li>('\"simply', 1)<li>('Sample', 1)<li>('results.txt', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('how', 2)<li>('analysis', 2)<li>('and', 41)<li>('much', 1)<li>('computing', 1)<li>('paradigm', 1)<li>('by', 13)<li>('Installing', 1)<li>('you', 14)<li>('Download', 1)<li>('2.', 2)<li>('available', 1)<li>('`tar', 1)<li>('provided,', 1)<li>('here', 5)<li>('follow', 2)<li>('standard', 2)<li>('Jupyter', 2)<li>('Notebook', 3)<li>('within', 1)<li>('directly.', 1)<li>('`$SPARKHOME/bin/pyspark`:', 1)<li>('local', 2)<li>('`sc.textFile`', 1)<li>('(which', 1)<li>('simply', 1)<li>('application.', 1)<li>('b)`', 1)<li>('functions.', 1)<li>('return', 5)<li>('sum(a,', 1)<li>('`flatmap`', 2)<li>('splits', 1)<li>('detailed', 1)<li>('Walk-Through).', 1)<li>('functions,', 2)<li>('separate', 2)<li>('write', 4)<li>('which', 2)<li>('program', 1)<li>('RDDs:', 1)<li>('\"movies\"', 1)<li>('Dataset', 1)<li>('any', 2)<li>('`functions.py`', 1)<li>('(several', 1)<li>('**Task', 16)<li>('dictionaries', 1)<li>('-', 15)<li>('tuple.', 1)<li>('lexicographically', 3)<li>('`reduceByKey`', 2)<li>('movieID', 1)<li>('Using', 1)<li>('movie.', 2)<li>('First', 1)<li>('null,', 1)<li>('average', 3)<li>('most', 1)<li>('rating.', 1)<li>('done', 2)<li>('(3,', 1)<li>('1163)]_', 1)<li>('Write', 2)<li>('`task10_flatmap`', 1)<li>('\"sanitization\"', 1)<li>('processing', 1)<li>('\"is', 1)<li>('\"s\".', 1)<li>('order.', 1)<li>(\"'i',\", 2)<li>(\"'scene',\", 1)<li>(\"'beatrice',\", 1)<li>('Nobel', 3)<li>('13', 1)<li>('were', 2)<li>('logs', 1)<li>(\"'01/Jul/1995'\", 1)<li>('self-explanatory,', 1)<li>('need:', 1)<li>('(provided', 1)<li>('Task', 1)<li>('recommendations.', 1)<li>('closest', 1)<li>('reality,', 1)<li>('measure).', 1)<li>('doable', 1)<li>('watched.', 1)<li>('ratingsRDD', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('#', 1)<li>('Spark', 12)<li>('this', 9)<li>('we', 13)<li>('use', 12)<li>('datasets', 2)<li>('in', 29)<li>('Started', 1)<li>('new', 3)<li>('Google', 1)<li>('based', 1)<li>('created', 1)<li>('these', 3)<li>('already', 1)<li>('But', 1)<li>('downloaded', 3)<li>('directory', 2)<li>('3.', 2)<li>('supports', 1)<li>('way', 2)<li>('\\tPYSPARK_PYTHON=/usr/bin/python3', 1)<li>('8881', 1)<li>('file.', 2)<li>('array', 1)<li>('rest', 1)<li>('Count', 2)<li>('command', 1)<li>('word:', 1)<li>('textfile.flatMap(split).map(generateone).reduceByKey(sum)', 1)<li>('(we', 2)<li>('representation', 1)<li>('compact', 1)<li>('out', 5)<li>('wordcount.py`', 1)<li>('\"Users\"', 1)<li>('(`se_posts.json`),', 1)<li>('Shakespeare', 1)<li>('examples', 1)<li>('16', 2)<li>('1', 1)<li>('form:', 3)<li>('ViewCount).', 1)<li>('Note', 3)<li>('`flatMap`', 2)<li>('extract', 2)<li>('title', 1)<li>('`join`', 1)<li>('that,', 1)<li>('element.', 1)<li>('moviesRDD,', 1)<li>('title,', 2)<li>('latter', 1)<li>('7:', 1)<li>('form', 2)<li>('movie).', 1)<li>('requests', 1)<li>('teh', 1)<li>('`(0,', 1)<li>('2268),', 1)<li>('(1,', 1)<li>('(3)', 1)<li>('replace', 1)<li>('\"is\\'t\"', 1)<li>('\"the\",', 1)<li>(\"'hero',\", 1)<li>('potentially', 1)<li>('anomalous', 1)<li>('behavior.', 1)<li>('[(154,', 1)<li>('(630,', 1)<li>('64,', 1)<li>('transformations', 1)<li>('returns', 2)<li>('print', 1)<li>('*dates*', 1)<li>('dates', 2)<li>('[NASA', 1)<li>('14', 1)<li>('days', 1)<li>('fetched', 2)<li>('user,', 2)<li>('Jaccard', 3)<li>('rated', 2)<li>('actual', 1)<li>('compare', 1)<li>('<=', 3)<li>('sets', 1)<li>('0.1270358306188925))_', 1)<li>('sequences', 1)<li>(\"'motivation'\", 1)<li>('`spark-submit`', 1)<li>('over).', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('data', 3)<li>('relatively', 2)<li>('small', 3)<li>('be', 22)<li>('found', 1)<li>('Spark](https://spark.apache.org)', 1)<li>('cluster', 1)<li>('2-stage', 1)<li>('system);', 1)<li>('items,', 1)<li>('variety', 1)<li>('or', 5)<li>('RDDs', 6)<li>('it', 14)<li>('package', 1)<li>('(modify', 1)<li>('`/spark`', 1)<li>('--', 7)<li>('Scala', 2)<li>('(Spark', 1)<li>('To', 2)<li>('\\t```', 2)<li>('--port=8881\"', 1)<li>('also', 5)<li>('about', 2)<li>('doing).', 1)<li>('initialized', 1)<li>('called', 1)<li>('`textFile`,', 1)<li>('reading', 1)<li>('output.', 1)<li>('without', 1)<li>('cluster.', 1)<li>('runs', 1)<li>('`$SPARKHOME/bin/spark-submit', 1)<li>('(`se_users.json`),', 1)<li>('\"Posts\"', 1)<li>('Noble', 1)<li>('few', 3)<li>('\"ratings\"', 1)<li>('RDDs.', 1)<li>('Before', 1)<li>('`filter`', 2)<li>('not', 4)<li>('so', 1)<li>('postsRDD', 1)<li>('2', 3)<li>('RDD.', 1)<li>('So', 3)<li>('appropriate', 3)<li>('contain', 2)<li>('first', 8)<li>('rated,', 1)<li>('2-tuple', 1)<li>('genre),', 1)<li>('6', 1)<li>(\"Let's\", 2)<li>(\"'mysql'),\", 1)<li>('Complete', 2)<li>('computes', 3)<li>('rating', 3)<li>('2.87)`', 1)<li>('(not', 2)<li>('followed', 3)<li>('previous', 2)<li>('100)`', 1)<li>('days),', 1)<li>('\",', 1)<li>('\"it', 2)<li>('\"\\'tis\"', 1)<li>('(4),', 1)<li>(\"'leonato',\", 2)<li>(\"'with',\", 1)<li>('9.341232227488153),', 1)<li>('PairRDD', 2)<li>('Make', 1)<li>('strings,', 1)<li>('entries', 1)<li>('On', 1)<li>(\"'cogroup'\", 1)<li>('15', 1)<li>('earlier', 1)<li>('neighbor', 1)<li>('ratings', 1)<li>('every', 1)<li>('100)', 1)<li>('collect', 1)<li>('consecutive', 1)<li>('sentence', 1)<li>('are\",', 1)<li>('its', 1)<li>('easier', 1)<li>('develop', 1)<li>('(by', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('easily', 1)<li>('###', 9)<li>('excellent', 2)<li>('tutorials', 1)<li>('[Spark', 2)<li>('website](http://spark.apache.org).', 1)<li>('developed', 1)<li>('open-source', 1)<li>('provides', 1)<li>('set', 2)<li>('more', 5)<li>('RDD,', 2)<li>('are', 14)<li>('distribution.', 1)<li>('manually:', 1)<li>('uncompress', 1)<li>('SPARKHOME=/data/Assignment-3/spark-3.5.0-bin-hadoop3/`', 1)<li>('Note:', 2)<li>('persist', 1)<li>('docker', 1)<li>('Python', 7)<li>('Scala),', 1)<li>('instructions', 1)<li>('languages.', 1)<li>('very', 1)<li>('verbose', 1)<li>('follow.', 1)<li>('(and', 1)<li>('--ip=0.0.0.0', 1)<li>('make', 2)<li>('just', 2)<li>('=', 2)<li>('(word,', 2)<li>('`lambda`', 2)<li>('b', 1)<li>('Tutorial](http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#Source+Code)', 1)<li>('tuples', 4)<li>('last', 1)<li>('MovieLens', 1)<li>('listing', 1)<li>('genres,', 1)<li>('appear', 2)<li>('regular', 1)<li>('`reduceByKey`.', 2)<li>('Ignore', 1)<li>('there', 2)<li>('4', 2)<li>('rated.', 4)<li>(\"['Adventure',\", 1)<li>(\"'1'\", 1)<li>('connect', 1)<li>('`ratingsRDD`,', 1)<li>('userID', 1)<li>('(title-word,', 1)<li>('value', 5)<li>('specific', 2)<li>('owneruserid', 1)<li>('tag)', 1)<li>('may', 2)<li>('[((7,', 1)<li>(\"'my.cnf'),\", 1)<li>('1)]_', 1)<li>('common', 2)<li>('9', 2)<li>('execute', 1)<li>('further', 1)<li>('ML).', 1)<li>('(2)', 1)<li>('stop', 1)<li>('it\",', 1)<li>('\"\\'twere\"', 1)<li>('after', 1)<li>(\"['act',\", 1)<li>('3.979695431472081),', 1)<li>('12', 1)<li>('surnames).', 1)<li>('class', 1)<li>('(if', 1)<li>(\"'02/Jul/1995').\", 1)<li>('details', 1)<li>('\"nearest', 1)<li>('neighbors\"', 1)<li>('incorporate', 1)<li>('Since', 1)<li>('100).', 1)<li>('100),', 1)<li>('\"sequences', 1)<li>('different', 1)<li>(\"Don't\", 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('to', 49)<li>('learn', 1)<li>('large-scale', 1)<li>('Spark:', 1)<li>('assignment,', 1)<li>(\"won't\", 2)<li>('distributed', 3)<li>('on', 13)<li>('Getting', 1)<li>('Berkeley.', 1)<li>('generalizes', 1)<li>('Map-Reduce', 2)<li>('Hadoop', 3)<li>('(RDDs)**.', 1)<li>('ways.', 1)<li>('one', 2)<li>('written', 2)<li>('chains', 1)<li>('ecosystem,', 1)<li>('if', 3)<li>('want', 3)<li>('`Assignment-5/`', 1)<li>('(so', 1)<li>('`export', 1)<li>('appropriately', 1)<li>('else).', 1)<li>('stuff', 2)<li>('equivalent', 1)<li>('code', 4)<li>('play', 3)<li>('PYSPARK_DRIVER_PYTHON=\"jupyter\"', 1)<li>('You', 8)<li>('variables', 1)<li>('textFile', 1)<li>('see', 3)<li>('prints', 1)<li>('5', 2)<li>('(in', 1)<li>('shell)', 1)<li>('does', 1)<li>('word', 3)<li>('count,', 2)<li>('i.e.,', 2)<li>('number', 8)<li>('+', 2)<li>('```', 2)<li>('`map`', 5)<li>('discuss', 1)<li>('Running', 1)<li>('file,', 2)<li>('contains', 4)<li>('Guide](https://spark.apache.org/docs/latest/programming-guide.html)', 1)<li>('manipulation', 1)<li>('should', 14)<li>('interfaces.', 1)<li>('table', 2)<li>('lines', 2)<li>('all', 17)<li>('tasks,', 1)<li>('require', 3)<li>('`null`', 1)<li>('atleast', 1)<li>('10000,', 1)<li>('has', 6)<li>('OwnerUserId,', 1)<li>('desired', 1)<li>('tuples.', 1)<li>('moviesRDD', 1)<li>('Genre),', 1)<li>('outputRDD', 2)<li>('(using', 1)<li>('title.', 1)<li>('smallest', 3)<li>('associated', 3)<li>('users', 5)<li>('function', 8)<li>('second', 3)<li>('key', 5)<li>('former', 1)<li>('`aggregateByKey`', 2)<li>('filter', 1)<li>('split', 1)<li>('reduceByKey', 1)<li>('appears.', 1)<li>('_Answer', 3)<li>(\"'schema'),\", 1)<li>('7', 1)<li>('takes', 3)<li>('`ratingsRDD`', 2)<li>('2-tuples', 1)<li>('`map`.', 1)<li>('`groupByKey`', 1)<li>('map', 1)<li>('movies,', 1)<li>('For', 5)<li>('(2,', 1)<li>('(4)', 1)<li>('(very', 1)<li>('were\",', 1)<li>('remove:', 1)<li>('\"is\",', 1)<li>('lines:', 1)<li>('10.', 1)<li>('userid', 2)<li>('_A', 1)<li>('211,', 1)<li>('etc),', 1)<li>('their', 1)<li>('values', 1)<li>('`take(5)`,', 1)<li>('lists).', 1)<li>('Logs](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)', 1)<li>('above),', 1)<li>('2-tuple,', 1)<li>('product', 1)<li>('restrict', 1)<li>('id', 2)<li>('u2', 1)<li>('highest', 1)<li>('comparing', 1)<li>('answer:', 1)<li>('simply\",', 1)<li>('application', 1)<li>('bigrams', 1)<li>('`motivation`s', 1)<li>('appear.', 1)<li>('File', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('Apache', 2)<li>('goal', 3)<li>('do', 8)<li>('using', 5)<li>('mode;', 1)<li>('however', 1)<li>('larger', 1)<li>('It', 2)<li>('**resilient', 1)<li>('collection', 1)<li>('operations', 2)<li>('operations.', 1)<li>('resource', 1)<li>('image', 2)<li>('http://spark.apache.org/downloads.html.', 1)<li>('using:', 1)<li>('spark-3.5.0-bin-hadoop3.tgz`', 1)<li>('`spark-3.5.0-bin-hadoop3/`.', 1)<li>('Set', 1)<li>('SPARKHOME', 2)<li>('5.', 1)<li>('move', 1)<li>('but', 9)<li>('when', 1)<li>('primarily', 1)<li>('three', 2)<li>('Java,', 1)<li>('start', 3)<li>('other', 4)<li>('Java', 2)<li>('shell.', 2)<li>('sure', 2)<li>('work.', 1)<li>('PySpark', 2)<li>('shell', 1)<li>('####', 1)<li>('counts', 2)<li>('`README.md`.', 1)<li>('\")).map(lambda', 1)<li>('generateone(word):', 1)<li>('b):', 1)<li>('`project5`', 1)<li>('try', 2)<li>('folllowing', 1)<li>('(`play.txt`)', 1)<li>('years', 1)<li>('Two', 1)<li>('(starting', 1)<li>('amount', 1)<li>('typically', 1)<li>('code.', 1)<li>(\"'OwnerUserId'\", 1)<li>(\"'viewcount'\", 1)<li>('viewCount', 1)<li>('contents', 1)<li>('movie', 3)<li>('`flatMap`s', 1)<li>('tags,', 1)<li>(\"'version-control'),\", 1)<li>('`mode`', 1)<li>('ties,', 1)<li>('`mode`.', 1)<li>('aggregating', 1)<li>('users.', 2)<li>('(the', 1)<li>('group', 1)<li>('3004),', 1)<li>('steps', 2)<li>('Specifically:', 2)<li>('lowercase,', 1)<li>('expand', 1)<li>('like', 2)<li>('\"do', 1)<li>(\"'and',\", 1)<li>('sequence', 1)<li>('`list`s,', 1)<li>('`logsRDD`.', 2)<li>('dates.', 1)<li>('minimize', 1)<li>('likely', 1)<li>(\"let's\", 1)<li>('100', 2)<li>(\"('4',\", 1)<li>('[Bigrams](http://en.wikipedia.org/wiki/Bigram)', 1)<li>('example,', 1)<li>('etc.', 1)<li>('task', 1)<li>('bigram', 1)<li>('Prizes', 1)<li>('copying', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('assignment', 2)<li>('for', 26)<li>('will', 14)<li>('the', 165)<li>('same', 3)<li>('summary', 1)<li>('that', 26)<li>('RDD', 16)<li>('transform', 1)<li>('file', 8)<li>('system', 1)<li>('manager.', 1)<li>('Docker', 1)<li>('includes', 1)<li>('up', 3)<li>('We', 7)<li>('3.5.0,', 1)<li>('3.3', 1)<li>('later**.', 1)<li>('create', 6)<li>('directory:', 1)<li>('interface', 1)<li>('--allow-root', 1)<li>('(it', 1)<li>('otherwise', 1)<li>('`>>>', 2)<li>('creates', 1)<li>('per', 3)<li>('some', 4)<li>('Application', 2)<li>('pyspark', 1)<li>('times', 3)<li>('each', 17)<li>('`counts.take(5)`', 1)<li>('def', 3)<li>('\")', 1)<li>('1)', 1)<li>('words,', 1)<li>('[Hadoop', 1)<li>('your', 1)<li>('*submit*', 1)<li>('encourage', 1)<li>('provided', 5)<li>('file:', 2)<li>('`spark_assignment.py`,', 1)<li>('JSON', 1)<li>('Laureates', 2)<li>('beginning', 1)<li>('`print(rdd.take(10))`).', 1)<li>('`task`).', 1)<li>('one-liners),', 1)<li>('find', 7)<li>('(None', 1)<li>('then', 4)<li>('Title,', 1)<li>('running', 1)<li>('`postsRDD.take(10)`', 1)<li>('If', 2)<li>('user', 8)<li>('across', 5)<li>('they', 8)<li>(\"'Fantasy']),\", 1)<li>('genre', 2)<li>('title-word', 1)<li>('couple', 1)<li>('count.', 1)<li>('compute', 3)<li>('`postsRDD`.', 1)<li>('pair', 1)<li>(\"`('1',\", 1)<li>('year.', 1)<li>('records', 1)<li>('3565),', 1)<li>('often', 1)<li>('text', 1)<li>('(1)', 1)<li>('common)', 1)<li>('words.', 2)<li>('(2),', 1)<li>('is\"', 1)<li>(\"'house',\", 1)<li>('11', 1)<li>('Specifically,', 1)<li>('post', 2)<li>('<', 1)<li>('`map`,', 1)<li>('(`physics`', 1)<li>('objects', 1)<li>('present', 1)<li>('entries.', 1)<li>('element', 2)<li>('day,', 1)<li>('day.', 1)<li>('did', 2)<li>('coefficient', 2)<li>('users,', 1)<li>('u1', 1)<li>('transformation', 1)<li>('_Example', 1)<li>('sequences\",', 1)<li>('of\",', 1)<li>('reason', 1)<li>('assume', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('tasks', 4)<li>('used', 4)<li>('programs', 1)<li>('datasets.', 1)<li>('guide', 2)<li>('originally', 1)<li>('UC', 1)<li>('significantly', 1)<li>('proposed', 1)<li>('into', 2)<li>('as', 15)<li>(\"'/data/Assignment-5'),\", 1)<li>('always', 2)<li>('quit', 1)<li>('image.', 1)<li>('Spark.', 1)<li>('Python.', 1)<li>('below', 1)<li>('do:', 3)<li>('$SPARKHOME/bin/pyspark', 1)<li>('bunch', 1)<li>('shell,', 2)<li>('`textFile.first()`,', 1)<li>('`textFile.take(5)`', 1)<li>('items', 1)<li>('recommend', 1)<li>('line.split(\"', 2)<li>('1)).reduceByKey(lambda', 1)<li>('b:', 1)<li>('split(line):', 1)<li>('counting', 3)<li>('description:', 1)<li>('large', 1)<li>('program,', 1)<li>('More...', 1)<li>('dictionary', 2)<li>('(`NASA_logs_sample.txt`)', 1)<li>('(`prize.json`)', 1)<li>('(e.g.,', 3)<li>('fill', 1)<li>('functions', 1)<li>('where', 8)<li>('python)', 1)<li>('(ID,', 2)<li>('movies', 11)<li>('num-movies).', 1)<li>('similar', 2)<li>('above,', 1)<li>('User', 1)<li>('1),', 2)<li>('correct', 2)<li>('8', 1)<li>('higher', 1)<li>('question,', 1)<li>('only', 1)<li>('aggregate.', 1)<li>('flatmap', 1)<li>('`playRDD`', 1)<li>('line,', 1)<li>('tokenization', 1)<li>('\"don\\'t\"', 1)<li>('\"', 1)<li>('remove', 1)<li>(\"'a',\", 1)<li>('>', 1)<li>('posts,', 1)<li>('`filter`.', 1)<li>('(26626,', 1)<li>('2.734375)]_', 1)<li>('`prizeRDD`', 1)<li>('\"hosts\"', 1)<li>('end', 1)<li>('URLs', 2)<li>('looking', 1)<li>('ignore', 1)<li>('distance', 1)<li>(\"('57',\", 1)<li>('Prize).', 1)<li>('bigram,', 1)<li>('`motivations`', 1)<li>('`pyspark`', 1)</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>('4:', 1)<li>('can', 15)<li>('with', 20)<li>('This', 7)<li>('a', 58)<li>('framework,', 1)<li>('popularized', 1)<li>('abstraction', 1)<li>('An', 6)<li>('including', 1)<li>('YARN', 1)<li>('spark', 2)<li>('Pre-built', 1)<li>('Move', 1)<li>('zxvf', 1)<li>('4.', 2)<li>('variable:', 1)<li>('somewhere', 1)<li>('tutorial', 1)<li>('(http://spark.apache.org/docs/latest/quick-start.html)', 1)<li>('hard', 1)<li>('shows', 1)<li>('through', 2)<li>('provided),', 1)<li>('need', 4)<li>('mapping', 1)<li>('port', 1)<li>('what', 1)<li>('from', 12)<li>('containing', 2)<li>('entry', 1)<li>('information', 2)<li>('doing', 1)<li>('`textFile.count()`', 1)<li>('Here', 3)<li>('Word', 2)<li>('textFile.flatMap(lambda', 1)<li>('line:', 1)<li>('a,', 1)<li>('`reduce`', 1)<li>('(look', 1)<li>('better', 1)<li>('definitions.', 1)<li>('Instead', 1)<li>('`wordcount.py`,', 1)<li>('commands.', 1)<li>('Stackexchange', 2)<li>('log', 4)<li>('pertaining', 1)<li>('over', 2)<li>('(https://grouplens.org/datasets/movielens/)', 1)<li>('Your', 2)<li>('posts', 2)<li>('our', 1)<li>('genres', 7)<li>('3', 1)<li>('year,', 1)<li>('expression),', 1)<li>(\"('1',\", 1)<li>('assuming', 1)<li>('those', 2)<li>('list', 8)<li>('(user,', 1)<li>('2),', 1)<li>('answer).', 2)<li>('either', 1)<li>('(i.e.,', 3)<li>('pick', 1)<li>('unlike', 1)<li>('`logsRDD`,', 2)<li>('day', 2)<li>('sample', 2)<li>('operates', 2)<li>('words', 2)<li>('non-alphanumerical', 1)<li>('\"in\",', 1)<li>(\"'enter',\", 1)<li>(\"'messenger']_\", 1)<li>('`reduceByKey`,', 1)<li>('well', 2)<li>('`category`', 1)<li>('final', 1)<li>('creating.', 1)<li>('given', 3)<li>('RDD:', 1)<li>('Cartesian', 1)<li>('who', 1)<li>('computation,', 1)<li>('bigrams:', 1)<li>('\"Bigrams', 1)<li>('\"are', 1)<li>('many', 1)<li>('present.', 1)<li>('`spark_assignment.py`', 1)</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0xffff82943a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "DisplayRDD(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
