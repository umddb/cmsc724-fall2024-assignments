{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa9143c",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The most basic form of a dataset we have is a collection of Objects (done as a `list` here). This class will support a few operations (we will add more below):\n",
    "1. `map`: this takes in a function as input, and applies that function to each object in the collection, and returns the resulting collection.\n",
    "1. `filter`: this also takes in a Boolean function, and returns the subset of objects that satisfies the condition.\n",
    "1. `cartesianproduct`: this takes in another collection as input and returns 2-tuples where every object for first collection is combined with every object from the other collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1590c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data):\n",
    "        assert isinstance(data, list), \"Data must be a list\"\n",
    "        self.data = data\n",
    "\n",
    "    def map(self, function):\n",
    "        l = [function(x) for x in self.data]  ## can use Python's map to do this as well\n",
    "        return Dataset(l)\n",
    "\n",
    "    def filter(self, function):\n",
    "        l = [x for x in self.data if function(x)]  ## can use Python's filter to do this as well\n",
    "        return Dataset(l)\n",
    "\n",
    "    def cartesianproduct(self, other):\n",
    "        joined_data = [(self_item, other_item) for self_item in self.data \n",
    "                                               for other_item in other.data]\n",
    "        return Dataset(joined_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140beb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset([1, 2, 3, 4])\n",
    "def f1(x):\n",
    "    return x*2\n",
    "mapped_dataset = dataset.map(f1)\n",
    "print(mapped_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9df56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "mapped_dataset = dataset.map(lambda x: x * 2)\n",
    "print(mapped_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081eaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello world', 11), ('This is a test', 14)]\n"
     ]
    }
   ],
   "source": [
    "sentences = Dataset([\"Hello world\", \"This is a test\"])\n",
    "res = sentences.map(lambda x: (x, len(x)))\n",
    "print(res.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3847c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda x: x % 2 == 0)\n",
    "print(filtered_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7989fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 2, 3], 'a'), ([1, 2, 3], 'b'), ([1, 2, 3], 'c')]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = Dataset([[1, 2, 3]])\n",
    "dataset2 = Dataset(['a', 'b', 'c'])\n",
    "product = dataset1.cartesianproduct(dataset2)\n",
    "print(product.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa70f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "('# Assignment 0: Computing Environment\\n', 38)\n",
      "-----------\n",
      "('\\n', 1)\n",
      "-----------\n",
      "('Over the course of the semester, you will work with a variety of software packages, including PostgreSQL, Apache Spark, MongoDB, and others. Installing those\\n', 158)\n"
     ]
    }
   ],
   "source": [
    "with open('README.md') as f:\n",
    "    file_lines = f.readlines()\n",
    "lines = Dataset(file_lines)\n",
    "\n",
    "ret = lines.map(lambda x: (x, len(x)))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(\"-----------\")\n",
    "    print(ret.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e8c28",
   "metadata": {},
   "source": [
    "Next we will define flatMap and join. \n",
    "\n",
    "1. `flatMap`: similar to map in that the function is applied to every object in the collection. However, the function must return a `list` and the final output is a flattened union of all those lists.\n",
    "1. `join`: similar to `cartesianproduct`, but also takes in a function as input that can check an arbitrary condition on the two objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c377843",
   "metadata": {},
   "source": [
    "Next we will define a `reduce` method, which takes in a function as input and repeatadly applies it to pairs of items until there is a single value left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c5bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatMap(self, function):\n",
    "    # This is a more verbose way of writing this\n",
    "    ret = []\n",
    "    for x in self.data:\n",
    "        y = function(x)\n",
    "        ret += y\n",
    "    return Dataset(ret)\n",
    "\n",
    "def join(self, other, function):\n",
    "    joined_data = [(self_item, other_item) for self_item in self.data \n",
    "                                           for other_item in other.data \n",
    "                                           if function(self_item, other_item)]\n",
    "    return Dataset(joined_data)\n",
    "\n",
    "Dataset.flatMap = flatMap\n",
    "Dataset.join = join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea15b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "sentences = Dataset([\"Hello world\", \"This is a test\"])\n",
    "words = sentences.flatMap(lambda x: x.split(\" \"))\n",
    "print(words.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7d2c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['INFO', 'Server started.'], ['ERROR', 'Failed to connect to database.'], ['INFO', \"User 'admin' logged in.\"], ['ERROR', 'Timeout occurred processing request.']]\n"
     ]
    }
   ],
   "source": [
    "log_entries = [\n",
    "    \"2023-03-13 10:00:00 - INFO - Server started.\",\n",
    "    \"2023-03-13 10:15:00 - ERROR - Failed to connect to database.\",\n",
    "    \"2023-03-13 10:30:00 - INFO - User 'admin' logged in.\",\n",
    "    \"2023-03-13 10:45:00 - ERROR - Timeout occurred processing request.\"\n",
    "]\n",
    "dataset = Dataset(log_entries)\n",
    "\n",
    "formatted_logs = dataset.map(lambda x: x.split(\" - \")[1:])\n",
    "print(formatted_logs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece17ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Failed', 'connect', 'database.', 'Timeout', 'occurred', 'processing', 'request.']\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords(log_entry):\n",
    "    _, level, message = log_entry.split(\" - \")\n",
    "    if level == \"ERROR\":\n",
    "        # Simple example, in reality, you'd filter out common stop words more effectively\n",
    "        return [word for word in message.split() if word.lower() not in [\"to\", \"the\", \"a\", \"in\"]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "error_keywords = dataset.flatMap(extract_keywords)\n",
    "print(error_keywords.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a377b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Great Gatsby', 'F. Scott Fitzgerald'), ('Moby Dick', 'Herman Melville'), ('1984', 'George Orwell')]\n"
     ]
    }
   ],
   "source": [
    "json_books = [\n",
    "    '{\"id\": 1, \"title\": \"The Great Gatsby\", \"author\": \"F. Scott Fitzgerald\", \"genres\": [\"Fiction\", \"Classic\"]}',\n",
    "    '{\"id\": 2, \"title\": \"Moby Dick\", \"author\": \"Herman Melville\", \"genres\": [\"Fiction\", \"Adventure\", \"Classic\"]}',\n",
    "    '{\"id\": 3, \"title\": \"1984\", \"author\": \"George Orwell\", \"genres\": [\"Fiction\", \"Dystopian\", \"Political\"]}'\n",
    "]\n",
    "dataset = Dataset(json_books)\n",
    "\n",
    "import json\n",
    "\n",
    "title_author_pairs = dataset.map(lambda x: (json.loads(x)['title'], json.loads(x)['author']))\n",
    "print(title_author_pairs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a978ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fiction', 'Classic', 'Fiction', 'Adventure', 'Classic', 'Fiction', 'Dystopian', 'Political']\n"
     ]
    }
   ],
   "source": [
    "genres = dataset.flatMap(lambda x: json.loads(x)['genres'])\n",
    "print(genres.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4362e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fiction', 1), ('Classic', 1), ('Fiction', 1), ('Adventure', 1), ('Classic', 1), ('Fiction', 1), ('Dystopian', 1), ('Political', 1)]\n"
     ]
    }
   ],
   "source": [
    "res1 = genres.map(lambda x: (x, 1))\n",
    "print(res1.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bced19b",
   "metadata": {},
   "source": [
    "#### reduce\n",
    "Next we will define `reduce` which is like an aggregate but takes in a function that merges two items as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb94b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(self, function):\n",
    "    result = self.data[0]\n",
    "    for item in self.data[1:]:\n",
    "        result = function(result, item)\n",
    "    return result\n",
    "\n",
    "Dataset.reduce = reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cba0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset([1, 2, 3, 4])\n",
    "print(dataset.reduce(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d829e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction,Classic,Fiction,Adventure,Classic,Fiction,Dystopian,Political\n"
     ]
    }
   ],
   "source": [
    "res2 = genres.reduce(lambda x, y: \",\".join([x, y]))\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92db3cc",
   "metadata": {},
   "source": [
    "The below is a slightly better implementation that takes in a starting value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62727e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(self, function, initial=None):\n",
    "    if initial is None:\n",
    "        if not self.data:\n",
    "            raise ValueError(\"reduce() of empty dataset with no initial value\")\n",
    "        result = self.data[0]\n",
    "        for item in self.data[1:]:\n",
    "            result = function(result, item)\n",
    "    else:\n",
    "        result = initial\n",
    "        for item in self.data:\n",
    "            result = function(result, item)\n",
    "    return Dataset(joined_data)\n",
    "\n",
    "Dataset.reduce = reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc6314",
   "metadata": {},
   "source": [
    "### DatasetKV\n",
    "\n",
    "Let's move on to DatasetKV. This will be a subclass of the above, with the requirement that the objects in the dataset are 2-tuples, which will be interpreted as (key, value) pairs. We typically require keys to be hashable -- we won't enforce it in the code below, but some functions may not work.\n",
    "\n",
    "The first twp operations we will define on this will be: \n",
    "1. \"equiJoin\" -- here we will join on the \"keys\", i.e., we will pair up objects that have the same keys across the two collections\n",
    "1. \"groupByKey\", which simply groups the objects by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01196964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetKV(Dataset):\n",
    "    def __init__(self, data):\n",
    "        assert all(isinstance(item, tuple) and len(item) == 2 for item in data), \"Data must be a list of key-value pairs\"\n",
    "        super().__init__(data)\n",
    "\n",
    "    def groupByKey(self):\n",
    "        grouped = {}\n",
    "        for k, v in self.data:\n",
    "            if k in grouped:\n",
    "                grouped[k].append(v)\n",
    "            else:\n",
    "                grouped[k] = [v]\n",
    "        return DatasetKV([(k, grouped[k]) for k in grouped])\n",
    "\n",
    "    \n",
    "    ## This is a naive implementation that wouldn't scale to large values\n",
    "    ## At the least, a hash table should be built on other.data\n",
    "    def equiJoin(self, other):\n",
    "        joined_data = [(self_item[0], (self_item[1], other_item[1])) \n",
    "                                           for self_item in self.data \n",
    "                                           for other_item in other.data \n",
    "                                           if self_item[0] == other_item[0]]\n",
    "        return DatasetKV(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d97d678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fruit', ['apple', 'banana']), ('vegetable', ['carrot'])]\n"
     ]
    }
   ],
   "source": [
    "pairs = DatasetKV([('fruit', 'apple'), ('fruit', 'banana'), ('vegetable', 'carrot')])\n",
    "grouped = pairs.groupByKey()\n",
    "print(grouped.data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "554ef076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fiction', [1, 1, 1]), ('Classic', [1, 1]), ('Adventure', [1]), ('Dystopian', [1]), ('Political', [1])]\n"
     ]
    }
   ],
   "source": [
    "genres_grouped = DatasetKV(res1.data).groupByKey()\n",
    "print(genres_grouped.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e583838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 3)), ('a', (1, 4)), ('a', (3, 3)), ('a', (3, 4)), ('b', (2, 3))]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = DatasetKV([('a', 1), ('a', 3), ('b', 2)])\n",
    "dataset2 = DatasetKV([('a', 3), ('a', 4), ('c', 4), ('b', 3)])\n",
    "joined_dataset = dataset1.equiJoin(dataset2)\n",
    "print(joined_dataset.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253788ff",
   "metadata": {},
   "source": [
    "Note: There are a few choices for the way the join result is represented. \n",
    "1. As shown above, which mimics how SQL does it.\n",
    "1. However, we may also do: (('a', 1), ('a', 4)) for the first one -- this keeps both the original tuples and makes more sense for non-equijoins\n",
    "1. Another option is do grouping instead, where the result could look like: \n",
    "[('a', ( [1, 3], [3, 4] )), ('b', ( [2], [3] ))\n",
    "\n",
    "In the last one, we are essentially grouping both the relations by the key and then joining them while keeping groups as they are. This is called a \"cogroup\" in Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44541587",
   "metadata": {},
   "source": [
    "#### ReduceByKey and SemiJoin\n",
    "Next, let's add reduceByKey and a semijoin operation to DatasetKV.\n",
    "\n",
    "1. reduceByKey: This allows us to \"merge\" or \"aggregate\" or \"reduce\" all the entries in a group. In most implementations, you would have to pass in a `reduce` function that takes in two arguments and \"reduces/merges\" then into a single value. Both the arguments and the return value must be of the same type.\n",
    "\n",
    "2. semiJoin: A variation of join, which is really more like a `filter`, where we only keep those entries in `self.data` that have a match in `other.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e49db8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceByKey(self, function):\n",
    "    reduced = {}\n",
    "    for k, v in self.data:\n",
    "        if k in reduced:\n",
    "            reduced[k] = function(reduced[k], v)\n",
    "        else:\n",
    "            reduced[k] = v\n",
    "    return DatasetKV([(k, reduced[k]) for k in reduced])\n",
    "\n",
    "def semijoin(self, other):\n",
    "    other_keys = set(k for k, _ in other.data)  # Extract keys from the other dataset.\n",
    "    filtered_data = [(k, v) for k, v in self.data if k in other_keys]\n",
    "    return DatasetKV(filtered_data)\n",
    "\n",
    "DatasetKV.reduceByKey = reduceByKey\n",
    "DatasetKV.semijoin = semijoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ade86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('January', 300), ('February', 250)]\n"
     ]
    }
   ],
   "source": [
    "sales = DatasetKV([('January', 100), ('February', 150), ('January', 200), ('February', 100)])\n",
    "\n",
    "def reduce_function(x, y):\n",
    "    return x+y\n",
    "\n",
    "total_sales = sales.reduceByKey(lambda x, y: x + y)\n",
    "print(total_sales.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee132907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 3), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = DatasetKV([('a', 1), ('a', 3), ('b', 2), ('d', 1)])\n",
    "dataset2 = DatasetKV([('a', 3), ('a', 4), ('c', 4), ('b', 3)])\n",
    "semijoined_dataset = dataset1.semijoin(dataset2)\n",
    "print(semijoined_dataset.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da5d4c",
   "metadata": {},
   "source": [
    "#### Lookup\n",
    "Finally, let's add a lookup function, which behaves like a \"left outer join\". For each entry in `self.data`, if there are matches in `other.data`, it will return all of them in a list. If there are no matches, an empty list will be returned. This is a common operation in MongoDB (only way to really do joins) and also in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a159b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(self, other):\n",
    "        joined_data = []\n",
    "\n",
    "        for k, v in self.data:\n",
    "            matches = [y[1] for y in other.data if y[0] == k]\n",
    "            joined_data.append(((k, (v, matches))))\n",
    "        \n",
    "        return DatasetKV(joined_data)\n",
    "\n",
    "DatasetKV.lookup = lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6907d02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, [3, 4])), ('a', (3, [3, 4])), ('b', (2, [3])), ('d', (3, []))]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = DatasetKV([('a', 1), ('a', 3), ('b', 2), ('d', 3)])\n",
    "dataset2 = DatasetKV([('a', 3), ('a', 4), ('c', 4), ('b', 3)])\n",
    "lookup_result = dataset1.lookup(dataset2) \n",
    "print(lookup_result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b045d",
   "metadata": {},
   "source": [
    "### DatasetTuple\n",
    "Next, we will define a Dataset of tuples for completeness. The functionality is largely the same as above, but because of the availability of \"schema\", we can do more structured stuff (e.g., joining on attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7408e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTuple(Dataset):\n",
    "    def __init__(self, data, schema):\n",
    "        assert all(isinstance(item, tuple) for item in data), \"Data must be a list of tuples\"\n",
    "        super().__init__(data)\n",
    "        self.schema = schema  # List of attributes\n",
    "\n",
    "    def join(self, other, attribute):\n",
    "        if attribute not in self.schema or attribute not in other.schema:\n",
    "            raise ValueError(\"Attribute not found in schema(s)\")\n",
    "        \n",
    "        self_index = self.schema.index(attribute)\n",
    "        other_index = other.schema.index(attribute)\n",
    "        \n",
    "        joined_data = []\n",
    "        for self_tuple in self.data:\n",
    "            for other_tuple in other.data:\n",
    "                if self_tuple[self_index] == other_tuple[other_index]:\n",
    "                    joined_data.append((self_tuple, other_tuple))\n",
    "        \n",
    "        return DatasetTuple(joined_data, self.schema + other.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = DatasetTuple([(1, 'apple'), (2, 'banana')], ['id', 'name'])\n",
    "dataset2 = DatasetTuple([(2, 'yellow'), (1, 'red')], ['id', 'color'])\n",
    "joined_dataset = dataset1.join(dataset2, 'id')\n",
    "print(joined_dataset.data)  # Output: [((1, 'apple'), (1, 'red')), ((2, 'banana'), (2, 'yellow'))]\n",
    "print(joined_dataset.schema)  # Output: ['id', 'name', 'id', 'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c89fd5",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bc66348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "dataset = Dataset([1, 2, 3])\n",
    "squared = dataset.map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "dataset = Dataset([1, 2, 3, 4])\n",
    "evens = dataset.filter(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17326d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "dataset = Dataset([1, 2, 3, 4])\n",
    "sum_result = dataset.reduce(lambda acc, x: acc + x, 0)  # Assuming a reduce method exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9ba0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO', 'WORLD']\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "dataset = Dataset([\"hello\", \"world\"])\n",
    "uppercase = dataset.map(str.upper)\n",
    "print(uppercase.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf3eee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'bye']\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "dataset = Dataset([ [\"hi\"], [\"bye\"]])\n",
    "characters = dataset.flatMap(lambda x: list(x))\n",
    "print(characters.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eadeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "dataset = Dataset([\"hello\", \"a\", \"world\"])\n",
    "long_strings = dataset.filter(lambda x: len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1c763ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book A', 'Book B']\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "json_dataset = Dataset(['{\"title\": \"Book A\", \"author\": \"Author X\"}', '{\"title\": \"Book B\", \"author\": \"Author Y\"}'])\n",
    "titles = json_dataset.map(lambda x: json.loads(x)['title'])\n",
    "print(titles.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7cfe2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book A', 'Author X', 'Book B', 'Author Y']\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "json_dataset = Dataset(['{\"title\": \"Book A\", \"author\": \"Author X\"}', '{\"title\": \"Book B\", \"author\": \"Author Y\"}'])\n",
    "all_info = json_dataset.flatMap(lambda x: list(json.loads(x).values()))\n",
    "print(all_info.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetkv = DatasetKV([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "grouped = datasetkv.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetkv = DatasetKV([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "reduced_sum = datasetkv.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = DatasetKV([(\"a\", 1), (\"b\", 2)])\n",
    "dataset2 = DatasetKV([(\"a\", 3), (\"c\", 4)])\n",
    "semijoin_result = dataset1.semijoin(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = DatasetTuple([(1, \"apple\"), (2, \"banana\")], [\"id\", \"name\"])\n",
    "dataset2 = DatasetTuple([(2, \"yellow\"), (1, \"red\")], [\"id\", \"color\"])\n",
    "joined_on_id = dataset1.join(dataset2, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset = Dataset(['{\"id\": 1, \"active\": true}', '{\"id\": 2, \"active\": false}'])\n",
    "active_ids = json_dataset.map(lambda x: json.loads(x)['id']).filter(lambda id: id > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731371c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset = Dataset(['{\"name\": \"John\", \"tags\": [\"friend\", \"school\"]}', '{\"name\": \"Doe\", \"tags\": [\"work\", \"gym\"]}'])\n",
    "tags = json_dataset.flatMap(lambda x: json.loads(x)['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23382240",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset = Dataset(['{\"name\": \"John\", \"tags\": [\"friend\", \"school\"]}', '{\"name\": \"Doe\", \"tags\": [\"work\", \"gym\"]}'])\n",
    "tags = json_dataset.flatMap(lambda x: json.loads(x)['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = Dataset([\"user1\", \"user2\"])\n",
    "dataset2 = Dataset([\"data1\", \"data2\"])\n",
    "joined_data = dataset1.join(dataset2).map(lambda x: f\"{x[0]}_info: {x[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726926fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
